{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3ee501",
   "metadata": {},
   "source": [
    "# Symantical Evaluation: Resume & Job Description\n",
    "\n",
    "\n",
    "## LangExtract & PDFLoader\n",
    "### LangExtract\n",
    "A Google solution that was optimised for long documents to **extract key entities with few-shot examples from unstructured data** (PDF in our case) adapting LLM to the job-seeking domain. It overcomes the \"needle-in-a-haystack\" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall, you can read more about LangExtract [here](https://github.com/google/langextract/tree/main). \n",
    "\n",
    "LangExtract will be use to extract the following entities **from key sections** within `job-description` and `resume`:\n",
    "**Entities Extracted**  \n",
    "- **Resume (`r`)**  \n",
    "  1. Hard skills  \n",
    "  2. Soft skills  \n",
    "  \n",
    "- **Job Description (`job_d`)**  \n",
    "  1. Hard skills  \n",
    "  2. Soft skills  \n",
    "  3. Contact person  \n",
    "  4. Years of experience  \n",
    "\n",
    "**Alternative to LangExtract**: An alternative is [SetFit](https://huggingface.co/blog/setfit), which can be trained with minimal examples to achieve RoBERTa-level performance.  \n",
    "- **Process:** Create a dataset with labeled soft skills (matching the ontology below), paired with job-description and resume sections.  \n",
    "- **Benefit:** Few-shot learning, efficient fine-tuning, and high performance with small data.  \n",
    "\n",
    "\n",
    "### PDFLoader\n",
    "PDFLoader is used to parse resumes and job descriptions into structured sections:\n",
    "\n",
    "- **Resume (`r`)**  \n",
    "  1. About / Summary  \n",
    "  2. Education  \n",
    "  3. Skills / Qualifications  \n",
    "  4. Professional Experience  \n",
    "  5. Certificates  \n",
    "  6. Volunteering  \n",
    "  7. Personal Projects  \n",
    "\n",
    "- **Job Description (`job_d`)**  \n",
    "  1. Responsibility  \n",
    "  2. Requirements / Qualifications / Experience  \n",
    "  3. Role Description  \n",
    "  4. Contact Person  \n",
    "\n",
    "---\n",
    "\n",
    "## Hybrid Solution: Sentence Transformers & LLM\n",
    "We evaluate resumes against job descriptions by comparing aligned sections extracted via PDFLoader and LangExtract.\n",
    "To evaluate resume with a job-description, we will compare sections from Resume (`r`) and the Job Description (`job_d`):\n",
    "\n",
    "#### 1. PDFLoader-Based Evaluation\n",
    "1. `job_d.Responsibility ↔ r.Professional Experience`: An LLM evaluates whether the candidate’s professional experience matches the job responsibilities.  \n",
    "   - **Guiding question:** *Does the candidate’s experience demonstrate the capability to deliver on the company’s stated responsibilities?*  \n",
    "   - **Scoring:** The model outputs a score between `0` and `1` within a predefined range:  \n",
    "     - `0 – 0.4`: Negative indication, results within this range indicates a missmatch between the professional experience of the candidate and the responsibility stated (could also indicates on commertial missmatch).\n",
    "     - `0.4 – 0.6`: Neutral indication, results within this range indicates that the LLM wasn't able to explicitly determnine an answer to the question.   \n",
    "     - `0.6 – 1.0`: Positive indication, results within this range indicate a match between the professional experience and the responsibilities stated.\n",
    "2. **Job description role title**: Given a title extracted, we will compute the cosine similarity between the pre-defined role-lists (a list of roles JobAgent will require the user to supply) embeddings and the job-description title embedding to determine whether or not the role-title match the candidate search by setting up an alpha (threshold). \n",
    "\n",
    "\n",
    "### 2. LangExtract-Based Evaluation\n",
    "1. **Hard Skills Evaluation**:\n",
    "    - `job_d.Requirements/Qualifications/Experience ↔ r.Skills`: Using the **hard skills** extracted by `LangExract`, we will compute a **hard skill score** by calculating the intersection of candidate skills with the required hard skills, then dividing by the total number of hard skills listed in the job description.\n",
    "\n",
    "2. **Soft Skills Evaluation**:\n",
    "  LangExtract maps soft skills from both resumes and job descriptions sections into a **predefined soft skill ontology** (can be found in `./soft_skills.json`). <br>\n",
    "  For each mapped section, we perform semantic similarity evaluation. Each ontology category is weighted, and the **dot product** is computed to obtain the final `soft_skill_score`.\n",
    "    - **Resume Sections:** About / Summary, Professional Experience, Education, Certificates, Volunteering  \n",
    "    - **Job Description Sections:** Responsibility, Requirements / Qualifications / Experience, Role Description\n",
    "  \n",
    "    To evalute the symantic similarity we will use a `sentence transformer` (bi-directional encoder) model [TechWolf/JobBERT-v2](https://huggingface.co/TechWolf/JobBERT-v2) which was specifically trained on 5M+ job-pairs. The model maps job titles and descriptions to a 1024-dimensional dense vector space and can be used for semantic job title matching, job similarity search, and related HR/recruitment tasks.\n",
    "\n",
    "3. **Years of experience**: Given the unique years extracted from a resume or years of experience extracted from job-description, we will evaluate experience of a candidate is within an acceptible predefined margin (gap).\n",
    "    - An acceptible marignal gap is : `Gap <= 2`. \n",
    "\n",
    "4. **Contact person**: Given the job description, if a contact person was found and candidate is interasted in the job, JobAgent will compose an persinlised email, otherwise JobAgent will compose a general email. The email will be composed taking into account the following context: \n",
    "    - The overall evaluation score (LangExtract + PDFLoader)\n",
    "    - The context extracted from web (the company's about-page context and reviews)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a47785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c6c851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import langextract as lx\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import batch_to_device, cos_sim\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eaf35bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there! How can I help you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Gimini connection \n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"hi\"\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f350f4",
   "metadata": {},
   "source": [
    "## Parse PDF & Extract Resume Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3a5b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gal Beeri - Data Scientist\n",
      "                             Melbourne, VIC I 0474 369 551 I  galbeeri1@gmail.com\n",
      "            LinkedIn: https://www.linkedin.com/in/gal-beeri I Github: https://github.com/Kokolipa\n",
      "\n",
      "Summary\n",
      "\n",
      "Forward-thinking Data Scientist with experience in machine learning, LLMs, and cloud-based data\n",
      "platforms. Skilled in using AWS, Azure, Python, SQL, Power BI and cutting-edge AI techniques to\n",
      "design and implement scalable, data-driven solutions that solve complex problems. Results-oriented\n",
      "and passionate about empowering organisations to make data-driven decisions that drive business\n",
      "success.\n",
      "\n",
      "Skills\n",
      "\n",
      "•   Cloud & DevOps: AWS, Azure, Microsoft Fabric, CLI, Git, GitHub, GitLab, Docker, MLflow\n",
      "•   GenAI: Agents (ReAct, Router, Multi-Agents, Conversational), LLMs (PEFT, LoRA, Summarisation,\n",
      "    Prompt Engineering), RAG (Multi-source, Self-RAG, Self-query, Re-ranking, Hybrid, HyDE, Binary\n",
      "    Quantisation), NLP-based solutions\n",
      "•   Machine Learning: Regression, Classification, Unsupervised Clustering, Decision Trees, Boosting,\n",
      "    Random Forest, Support Vector Classifier, Anomaly Detection\n",
      "•   Python: LangChain, LangGraph, LangSmith, DeepEval, FastAPI, Pydantic, Streamlit, PyTorch,\n",
      "    TensorFlow, Keras, Scikit-learn, PyOD, NLTK, Transformers (HuggingFace), Pandas, Numpy,\n",
      "    SciPy, PySpark, SQLAlchemy, BeautifulSoup, Splinter, Flask, Pytest\n",
      "•   Databases: SQLite, MySQL, PostgreSQL, NoSQL (MongoDB), REDCap\n",
      "•   Analytics & Visualisation: Power BI, Power Query, Tableau, Matplotlib, Seaborn, Dash, Plotly\n",
      "•   Other Tools & Languages: SQL, DAX, M, Bash, HTML5, JavaScript, Confluence, Jira, Draw.io\n",
      "Professional Experience\n",
      "\n",
      "Data Scientist at SkillField       (AGL Energy Project), Melbourne, VIC                   Apr 2025 – Aug 2025\n",
      "\n",
      "•   Refactored multiple Python scripts to prepare, pre          -process, transform, and feature engineer big data\n",
      "    cutting Azure ML runtime significantly.\n",
      "•   Developed a method to estimate energy appliance usage and benchmark                      ed performance against\n",
      "    existing models.\n",
      "•   Improved CI/CD workflows and Python code leveraging threading and asynchronous best practices.\n",
      "•   De-cluttered the GitHub repository with Bash scripts and contributed to the development of client’s\n",
      "    in-house Python package.\n",
      "Cloud tools: Azure (Blob Storage, Machine Learning, CLI)\n",
      "Data Scientist and Engineer at DataDivers.io, Perth, WA                                             Feb 2024 – Mar 2025\n",
      "Delivered multiple AI/ML projects across education            and health sectors, including:\n",
      "•   Origins GenAI Project: Refactored state-of-the-art models used for Name Entity Recognition\n",
      "    (NER) and Normalisation (NEN) to tag ontological entities. Developed a RAG solution to enhance\n",
      "    NEN performance and accuracy. Prepared datasets and fine-tuned the LLM to optimise\n",
      "    performance and contributed to the published research paper (https://arxiv.org/abs/2410.20695).\n",
      "•   Student Engagement Engine: Built an end-to-end predictive classification dropout-risk model for a\n",
      "    university. Designed and implemented a data pipeline with automated retraining, model drift\n",
      "    detection, backup creation       and dashboard delivery.\n"
     ]
    }
   ],
   "source": [
    "resume_path: str = \"~/Desktop/Private/CVs/version_new/Gal Beeri Data Science CV.pdf\"\n",
    "loader = PyPDFLoader(\n",
    "    file_path=resume_path,\n",
    "    extract_images=False,\n",
    "    mode=\"page\",\n",
    "    extraction_mode=\"layout\"\n",
    ")\n",
    "resume = loader.load()\n",
    "resume_pages: List[str] = [page.page_content for page in resume]\n",
    "print(resume_pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97293b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Summary': 'Forward-thinking Data Scientist with experience in machine learning, LLMs, and cloud-based data\\nplatforms. Skilled in using AWS, Azure, Python, SQL, Power BI and cutting-edge AI techniques to\\ndesign and implement scalable, data-driven solutions that solve complex problems. Results-oriented\\nand passionate about empowering organisations to make data-driven decisions that drive business\\nsuccess.\\n\\nSkills',\n",
       " 'Skills': '•   Cloud & DevOps: AWS, Azure, Microsoft Fabric, CLI, Git, GitHub, GitLab, Docker, MLflow\\n•   GenAI: Agents (ReAct, Router, Multi-Agents, Conversational), LLMs (PEFT, LoRA, Summarisation,\\n    Prompt Engineering), RAG (Multi-source, Self-RAG, Self-query, Re-ranking, Hybrid, HyDE, Binary\\n    Quantisation), NLP-based solutions\\n•   Machine Learning: Regression, Classification, Unsupervised Clustering, Decision Trees, Boosting,\\n    Random Forest, Support Vector Classifier, Anomaly Detection\\n•   Python: LangChain, LangGraph, LangSmith, DeepEval, FastAPI, Pydantic, Streamlit, PyTorch,\\n    TensorFlow, Keras, Scikit-learn, PyOD, NLTK, Transformers (HuggingFace), Pandas, Numpy,\\n    SciPy, PySpark, SQLAlchemy, BeautifulSoup, Splinter, Flask, Pytest\\n•   Databases: SQLite, MySQL, PostgreSQL, NoSQL (MongoDB), REDCap\\n•   Analytics & Visualisation: Power BI, Power Query, Tableau, Matplotlib, Seaborn, Dash, Plotly\\n•   Other Tools & Languages: SQL, DAX, M, Bash, HTML5, JavaScript, Confluence, Jira, Draw.io\\nProfessional Experience',\n",
       " 'Professional Experience': 'Data Scientist at SkillField       (AGL Energy Project), Melbourne, VIC                   Apr 2025 – Aug 2025\\n\\n•   Refactored multiple Python scripts to prepare, pre          -process, transform, and feature engineer big data\\n    cutting Azure ML runtime significantly.\\n•   Developed a method to estimate energy appliance usage and benchmark                      ed performance against\\n    existing models.\\n•   Improved CI/CD workflows and Python code leveraging threading and asynchronous best practices.\\n•   De-cluttered the GitHub repository with Bash scripts and contributed to the development of client’s\\n    in-house Python package.\\nCloud tools: Azure (Blob Storage, Machine Learning, CLI)\\nData Scientist and Engineer at DataDivers.io, Perth, WA                                             Feb 2024 – Mar 2025\\nDelivered multiple AI/ML projects across education            and health sectors, including:\\n•   Origins GenAI Project: Refactored state-of-the-art models used for Name Entity Recognition\\n    (NER) and Normalisation (NEN) to tag ontological entities. Developed a RAG solution to enhance\\n    NEN performance and accuracy. Prepared datasets and fine-tuned the LLM to optimise\\n    performance and contributed to the published research paper (https://arxiv.org/abs/2410.20695).\\n•   Student Engagement Engine: Built an end-to-end predictive classification dropout-risk model for a\\n    university. Designed and implemented a data pipeline with automated retraining, model drift\\n    detection, backup creation       and dashboard delivery.\\nData Analyst at SpendAble           (FinTech Startup), Melbourne, VIC                               Aug 2021 - Nov 2022\\n\\n•   Designed customer insight dashboards, boosting retention and renewal rates.\\n•   Conducted classification modelling and automated API messaging to reduce churn.\\n•   Built a customer experience framework to measure adoption and engagement.\\n•   Automated financial reconciliation pipelines.\\n',\n",
       " 'Education': '•   Data Analytics Certificate, Monash University, Melbourne, Australia                             May 2023 - Nov 2023\\n    Course Average: 97.4%\\n•   Bachelor of Law (LLB), Netanya Academic College, Israel                                          Feb 2017 - Feb 2021\\n    Honours List (all years)\\nProfessional Certifications',\n",
       " 'Professional Certifications': '•   Natural Language Processing (NLP), Maven Analytics                                                              Aug 2025\\n•   Azure Machine Learning, Microsoft                                                                               May 2025\\n•   Fabric Analytics Engineer, Microsoft                                                                             Jan 2025\\n•   Data Visualization with Matplotlib & Seaborn, Maven Analytics                                            Nov 2024\\n•   Ultimate AWS Certified Cloud Practitioner CLF-C02 2025, Udemy                                                   Sep 2024\\n•   Data Science in Python: Classification, Maven Analytics                                                          Feb 2024\\n•   Data Science in Python: Regression, Data Prep & EDA, Maven Analytics                                             Jan 2024\\n•   My SQL Data Analysis (CPE Credit), Maven Analytics                                                          Jan 2024\\n•   Power BI Desktop (CPE Credit), Maven Analytics                                                                 Nov 2023\\nPersonal Projects & Volunteering',\n",
       " 'Personal Projects & Volunteering': '•   CleanData Python Data Cleansing Library (2024) – Designed and developed a Python library to\\n    streamline data preprocessing, including memory optimisation, missing value handling (treats MAR,\\n    MCAR, and MNAR cases) duplicate and anomaly detection, text cleaning, and AI-powered table\\n    exploration with TAPAS.\\n\\n•   AI Camp, Melbourne (2024 – Present) – Supporting live events showcasing AI/ML technologies\\n    and best practices.\\n•   YAICC, Melbourne (2021 & 2023) – Mentored a university student aspiring to enter technology and\\n    business roles.\\n•   StartSpace, Melbourne (2021) – Participated in a startup incubator, sharing ideas and processes\\n    with peers.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define regex patterns: \n",
    "dot_points_pattern: str = r\"^•\\s+\"\n",
    "job_experience_pattern: str = r\"^(?!•)(.+?\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\\s*[-–]\\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)?\\s*\\d{0,4})$\"\n",
    "# Define a list with section names\n",
    "sections: List[str] = [\n",
    "    \"Summary\",\n",
    "    \"Skills\",\n",
    "    \"Professional Experience\",\n",
    "    \"Education\",\n",
    "    \"Professional Certifications\",\n",
    "    \"Personal Projects & Volunteering\"\n",
    "]\n",
    "resume_dict: Dict[str, str] = {}\n",
    "\n",
    "for i, page in enumerate(resume_pages, 1):\n",
    "    # Extract min and max spans: Reset the spans for each page iterated\n",
    "    max_spans: List[int] = [] \n",
    "    min_spans: List[int] = []\n",
    "    max_span: int = 0\n",
    "    for section in sections: \n",
    "        results = re.finditer(pattern=section, string=page, flags=re.DOTALL)\n",
    "        for result in results:\n",
    "            max_spans.append(result.span()[1]) # end span\n",
    "            max_span = result.span()[1] \n",
    "            min_spans.append(result.span()[0])\n",
    "    \n",
    "    # Get the size of the sections in the page\n",
    "    min_span = min(min_spans)\n",
    "    sections_size = len(max_spans)\n",
    "    for idx, span in enumerate(max_spans):\n",
    "        if i == 1: \n",
    "            # Extract sections and clean up results\n",
    "            if span != max_span:\n",
    "                section_result = page[max_spans[idx]:max_spans[idx+1]].strip(\"\\n\\n\").strip()\n",
    "        \n",
    "                # section_result = re.sub(pattern=dot_points_pattern, repl=\" \", string=section_result, flags=re.MULTILINE)\n",
    "                resume_dict[sections[idx]] = section_result\n",
    "            else:  \n",
    "                section_result = page[span:].strip(\"\\n\\n\").strip()\n",
    "                \n",
    "                # section_result = re.sub(pattern=dot_points_pattern, repl=\" \", string=section_result, flags=re.MULTILINE).strip(\"\\n\\n\").strip()\n",
    "                resume_dict[sections[idx]] = section_result\n",
    "        else: \n",
    "            if span != max_span:\n",
    "                section_result = page[max_spans[idx]:max_spans[idx+1]].strip(\"\\n\\n\").strip()\n",
    "                \n",
    "                # section_result = re.sub(pattern=dot_points_pattern, repl=\" \", string=section_result, flags=re.MULTILINE).strip(\"\\n\\n\").strip()\n",
    "                resume_dict[sections[idx + sections_size]] = section_result\n",
    "            else:\n",
    "                # Extract additional job experience from the second page \n",
    "                job_exper = [match for match in re.finditer(pattern=job_experience_pattern, string=page, flags=re.MULTILINE)]\n",
    "                additional_experience = job_exper[0].span()[0]\n",
    "                resume_dict[\"Professional Experience\"] = resume_dict.get(\"Professional Experience\") + \"\\n\" + page[additional_experience:min_span]\n",
    "                \n",
    "                section_result = page[span:].strip(\"\\n\\n\").strip()\n",
    "                resume_dict[sections[idx + sections_size]] = section_result\n",
    "\n",
    "\n",
    "# Print the resulted dictionary\n",
    "resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b318628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_years_of_experience(resume_obj: Dict[str, str], key: str): \n",
    "    experiences = resume_dict[key]\n",
    "    # Extract and parse dates from resume \n",
    "    years_pattern = r\"\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)[-\\s/]*(?:19|20)\\d{2}\\b\"\n",
    "    year_month_format = \"%b %Y\"\n",
    "    year_months = [match for match in re.findall(pattern=years_pattern, string=experiences, flags=re.IGNORECASE)]\n",
    "    parse_dates = [datetime.strptime(year, year_month_format) for year in year_months]\n",
    "\n",
    "    return round((max(parse_dates) - min(parse_dates)).days / 365, 2)\n",
    "\n",
    "get_years_of_experience(resume_dict, \"Professional Experience\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63d888",
   "metadata": {},
   "source": [
    "## Define Extraction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a281a6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'presented to stakeholders',\n",
       "  'extractions': [{'extraction_text': 'presented to stakeholders',\n",
       "    'extraction_class': 'communication'}]},\n",
       " {'text': 'collaborated with cross-functional teams',\n",
       "  'extractions': [{'extraction_text': 'collaborated with cross-functional teams',\n",
       "    'extraction_class': 'communication'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_path: str = \"../src/data/soft_skills.json\"\n",
    "with open(document_path, \"r\") as file: \n",
    "    soft_skill_ontology: Dict[str, List[str]] = json.load(file)\n",
    "soft_skill_ontology[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e453498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExampleData(text='presented to stakeholders', extractions=[Extraction(extraction_class='communication', extraction_text='presented to stakeholders', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'softskill_type': 'communication'})]),\n",
       " ExampleData(text='collaborated with cross-functional teams', extractions=[Extraction(extraction_class='communication', extraction_text='collaborated with cross-functional teams', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'softskill_type': 'communication'})]),\n",
       " ExampleData(text='wrote technical documentation', extractions=[Extraction(extraction_class='communication', extraction_text='wrote technical documentation', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'softskill_type': 'communication'})]),\n",
       " ExampleData(text='led a team of', extractions=[Extraction(extraction_class='leadership', extraction_text='led a team of', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'softskill_type': 'leadership'})]),\n",
       " ExampleData(text='mentored junior developers', extractions=[Extraction(extraction_class='leadership', extraction_text='mentored junior developers', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'softskill_type': 'leadership'})])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prompt for LangExtract has to descrive the extraction class \n",
    "EXTRACTION_PROMPT: str = (\"\"\"\\\n",
    "Extract communication, leadership, problem solving, teamwork collaboration, analytical thinking, adaptability, creativity and innovation, and time management optimisation using attributes to group soft skills related information: \n",
    "1. Extract entities in the order they appear in the text.\n",
    "2. Use the exact text for extractions. Do not paraphrase or overlap entities.\n",
    "3. Soft skill groups can have different values but should always have the same key \"softskill_type\".\n",
    "\"\"\")\n",
    "\n",
    "# Create an object to store examples\n",
    "examples = []\n",
    "example_size: int = 3\n",
    "\n",
    "# Create an object to limit the amount of examples to be provided to LangExtract \n",
    "group_counts: Dict[str, int] = {}\n",
    "for example in soft_skill_ontology: \n",
    "    # Extract key elements from softskill ontology \n",
    "    extractions = example.get(\"extractions\")\n",
    "    text = example.get(\"text\")\n",
    "    extraction_class = example.get(\"extractions\")[0].get(\"extraction_class\")\n",
    "\n",
    "    # Get the current count for the group, defaulting to 0 if it's the first time\n",
    "    current_group_count = group_counts.get(extraction_class, 0)\n",
    "    if current_group_count < example_size: \n",
    "        group_counts[extraction_class] = current_group_count + 1\n",
    "        \n",
    "        # Formulate LangExtract examples  \n",
    "        for extraction in extractions:\n",
    "            extraction_text = extraction.get(\"extraction_text\")\n",
    "            examples.append(\n",
    "                lx.data.ExampleData(\n",
    "                    text=text,\n",
    "                    extractions=[\n",
    "                        lx.data.Extraction(\n",
    "                            extraction_class=extraction_class,\n",
    "                            extraction_text=extraction_text,\n",
    "                            attributes={\"softskill_type\": extraction_class}\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# Preview first 5 examples \n",
    "examples[:5]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ab7e3",
   "metadata": {},
   "source": [
    "## Extract Entities from Resume: Soft & Hard skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9815c",
   "metadata": {},
   "source": [
    "### Soft Skill extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_keys: List[str] = ['Summary', 'Professional Experience', 'Personal Projects & Volunteering']\n",
    "input_text: str = \"\\n\\n\".join([resume_dict[key] for key in resume_keys])\n",
    "\n",
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=EXTRACTION_PROMPT,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5fdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 28 entities:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92msoftskills.jsonl\u001b[0m: 1 docs [00:00, 874.18 docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92msoftskills.jsonl\u001b[0m\n",
      "Counter({ 'problem_solving': 9,\n",
      "          'creativity_innovation': 8,\n",
      "          'analytical_thinking': 3,\n",
      "          'teamwork_collaboration': 2,\n",
      "          'leadership': 2,\n",
      "          'communication': 2,\n",
      "          'time_management_organization': 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracted {len(result.extractions)} entities:\\n\")\n",
    "\n",
    "\n",
    "# Save as JSON Line\n",
    "lx.io.save_annotated_documents(\n",
    "    annotated_documents=[result],\n",
    "    output_dir=\"../src/data/\",\n",
    "    output_name=\"softskills.jsonl\"\n",
    ")\n",
    "\n",
    "# Softskills breakdown \n",
    "breakdown = Counter([extraction.extraction_class for extraction in result.extractions])\n",
    "pprint(breakdown, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "da82b405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f6581c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32653061224489793\n",
      "0.5625\n",
      "1.6530612244897962\n",
      "0.08163265306122448\n",
      "0.12755102040816327\n"
     ]
    }
   ],
   "source": [
    "overall_entities = len(result.extractions)\n",
    "for key, value in breakdown.items():\n",
    "    class_weight = value/overall_entities\n",
    "    weighted_avg = (class_weight * value) / overall_entities\n",
    "    print(weighted_avg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4db11ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92msoftskills.jsonl\u001b[0m: 100%|██████████| 12.0k/12.0k [00:00<00:00, 18.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92msoftskills.jsonl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the interactive visualization\n",
    "html_content = lx.visualize(\"../src/data/langextract/softskills/softskills.jsonl\")\n",
    "with open(\"../src/data/softskills.html\", \"w\") as f:\n",
    "    if hasattr(html_content, 'data'):\n",
    "        f.write(html_content.data) \n",
    "    else:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c53c60c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'developed and deployed applications using AWS services like EC2, S3, and Lambda.',\n",
       "  'extractions': [{'extraction_text': 'AWS',\n",
       "    'extraction_class': 'cloud_services'},\n",
       "   {'extraction_text': 'EC2', 'extraction_class': 'cloud_services'},\n",
       "   {'extraction_text': 'S3', 'extraction_class': 'cloud_services'},\n",
       "   {'extraction_text': 'Lambda', 'extraction_class': 'cloud_services'}]},\n",
       " {'text': 'architected solutions on Azure, leveraging Azure Functions and Cosmos DB for scalability.',\n",
       "  'extractions': [{'extraction_text': 'Azure',\n",
       "    'extraction_class': 'cloud_services'},\n",
       "   {'extraction_text': 'Azure Functions',\n",
       "    'extraction_class': 'cloud_services'},\n",
       "   {'extraction_text': 'Cosmos DB', 'extraction_class': 'cloud_services'}]}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load skills.json \n",
    "skills_path: str = \"../src/data/langextract/skills/skills.json\"\n",
    "with open(skills_path, \"r\") as file: \n",
    "    skills_json = json.load(file)\n",
    "skills_json[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a5e7bf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExampleData(text='developed and deployed applications using AWS services like EC2, S3, and Lambda.', extractions=[Extraction(extraction_class='cloud_services', extraction_text='AWS', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'skill_type': 'cloud_services'})]),\n",
       " ExampleData(text='developed and deployed applications using AWS services like EC2, S3, and Lambda.', extractions=[Extraction(extraction_class='cloud_services', extraction_text='EC2', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'skill_type': 'cloud_services'})]),\n",
       " ExampleData(text='developed and deployed applications using AWS services like EC2, S3, and Lambda.', extractions=[Extraction(extraction_class='cloud_services', extraction_text='S3', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'skill_type': 'cloud_services'})]),\n",
       " ExampleData(text='developed and deployed applications using AWS services like EC2, S3, and Lambda.', extractions=[Extraction(extraction_class='cloud_services', extraction_text='Lambda', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'skill_type': 'cloud_services'})]),\n",
       " ExampleData(text='architected solutions on Azure, leveraging Azure Functions and Cosmos DB for scalability.', extractions=[Extraction(extraction_class='cloud_services', extraction_text='Azure', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes={'skill_type': 'cloud_services'})])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the prompt to Extract skilss from resume with LangExtract\n",
    "EXTRACTION_PROMPT_SKILLS: str = (\"\"\"\\\n",
    "Extract cloud services, databases, dev languages, data visualisation, and algorithms using attributes to group skills related information: \n",
    "1. Extract entities in the order they appear in the text.\n",
    "2. Use the exact text for extractions. Do not paraphrase or overlap entities.\n",
    "3. Skill groups can have different values but should always have the same key \"skill_type\".\n",
    "\"\"\")\n",
    "\n",
    "# Create an object to store examples\n",
    "skill_examples = []\n",
    "\n",
    "for example in skills_json: \n",
    "    # Extract key elements from skills.json\n",
    "    extractions = example.get(\"extractions\")\n",
    "    text = example.get(\"text\")\n",
    "\n",
    "    if len(example.get(\"extractions\")) > 1:\n",
    "        for extraction in  example.get(\"extractions\"):\n",
    "            extraction_class = extraction.get(\"extraction_class\")\n",
    "            extraction_text = extraction.get(\"extraction_text\")\n",
    "            # Formulate LangExtract examples\n",
    "            skill_examples.append(\n",
    "                lx.data.ExampleData(\n",
    "                    text=text,\n",
    "                    extractions=[\n",
    "                        lx.data.Extraction(\n",
    "                            extraction_class=extraction_class,\n",
    "                            extraction_text=extraction_text,\n",
    "                            attributes={\"skill_type\": extraction_class}\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        extraction_class = extraction.get(\"extraction_class\")\n",
    "        extraction_text = extraction.get(\"extraction_text\") \n",
    "\n",
    "        # Formulate LangExtract examples\n",
    "        skill_examples.append(\n",
    "            lx.data.ExampleData(\n",
    "                text=text,\n",
    "                extractions=[\n",
    "                    lx.data.Extraction(\n",
    "                        extraction_class=extraction_class,\n",
    "                        extraction_text=extraction_text,\n",
    "                        attributes={\"skill_type\": extraction_class}\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "# View the first 5 examples\n",
    "skill_examples[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5aa72e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Prompt alignment: FAILED to align: [example#12] class='databases' status=None text='MongoDB'\n",
      "WARNING:absl:Prompt alignment: FAILED to align: [example#24] class='dev_language' status=None text='Kotlin'\n"
     ]
    }
   ],
   "source": [
    "skills_resume_keys: List[str] = ['Skills', 'Professional Experience', 'Professional Certifications', 'Personal Projects & Volunteering']\n",
    "input_text_skills: str = \"\\n\\n\".join([resume_dict[key] for key in skills_resume_keys])\n",
    "\n",
    "result_skills = lx.extract(\n",
    "    text_or_documents=input_text_skills,\n",
    "    prompt_description=EXTRACTION_PROMPT_SKILLS,\n",
    "    examples=skill_examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e4c50756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 91 entities:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mskills.jsonl\u001b[0m: 1 docs [00:00, 536.70 docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mskills.jsonl\u001b[0m\n",
      "Counter({ 'dev_language': 36,\n",
      "          'algorithms': 21,\n",
      "          'cloud_services': 16,\n",
      "          'data_visualisation': 10,\n",
      "          'databases': 8})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracted {len(result_skills.extractions)} entities:\\n\")\n",
    "\n",
    "# Save as JSON Line\n",
    "lx.io.save_annotated_documents(\n",
    "    annotated_documents=[result_skills],\n",
    "    output_dir=\"../src/data/langextract/skills\",\n",
    "    output_name=\"skills.jsonl\"\n",
    ")\n",
    "\n",
    "# Softskills breakdown \n",
    "breakdown = Counter([extraction.extraction_class for extraction in result_skills.extractions])\n",
    "pprint(breakdown, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "11e045a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mskills.jsonl\u001b[0m: 100%|██████████| 29.6k/29.6k [00:00<00:00, 15.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mskills.jsonl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualise with LangExtract html object \n",
    "html_content_skills = lx.visualize(\"../src/data/langextract/skills/skills.jsonl\")\n",
    "with open(\"../src/data/langextract/skills/skills.html\", \"w\") as f:\n",
    "    if hasattr(html_content_skills, 'data'):\n",
    "        f.write(html_content_skills.data) \n",
    "    else:\n",
    "        f.write(html_content_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a4763",
   "metadata": {},
   "source": [
    "## Extract Entities from Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6aa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
